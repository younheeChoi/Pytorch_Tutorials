{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blessed-nicholas",
   "metadata": {},
   "source": [
    "## 3.3 신경망 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-medicare",
   "metadata": {},
   "source": [
    "### 3.3.1 인공 신경망 (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-richards",
   "metadata": {},
   "source": [
    "### 3.3.2 간단한 분류 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-ready",
   "metadata": {},
   "source": [
    "#### 분류의 목적은 정답을 맞추는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extra-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이,skearn을 사용하여 array로 이용하려함\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-celtic",
   "metadata": {},
   "source": [
    "##### 학습 및  평가에 사용할 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greater-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 80개의 2차원 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "canadian-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 2\n",
    "x_train, y_train = make_blobs(n_samples=80, n_features = n_dim, centers=[[1,1], [-1,-1], [1,-1], [-1,1]]\n",
    "                              , shuffle = True, cluster_std=0.3)\n",
    "x_test, y_test = make_blobs(n_samples=20, n_features = n_dim, centers=[[1,1], [-1,-1], [1,-1], [-1,1]]\n",
    "                            , shuffle = True, cluster_std=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-maria",
   "metadata": {},
   "source": [
    "make_blobs 가상 데이터 생성 함수  \n",
    "n_samples = 표본데이터의 수 (default = 100)  \n",
    "n_features = 차원  \n",
    "centerts= 생성할 클러스터의 수 or 중심 ( [n_centers, n_features] 크기의 배열)  \n",
    "cluster_std = 클러스터의 표준편차  \n",
    "document: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "formal-dance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82662363,  0.72941722],\n",
       "       [ 1.24667867, -1.13637233],\n",
       "       [-1.1635424 , -0.92398973],\n",
       "       [ 1.58837023, -1.12443126],\n",
       "       [ 0.95665428, -0.92421483],\n",
       "       [ 0.74114388,  1.03890283],\n",
       "       [ 1.64951863,  0.76724772],\n",
       "       [-0.92310129,  1.31864702],\n",
       "       [-0.86159916, -1.33239423],\n",
       "       [ 0.94068651,  1.26831757],\n",
       "       [-0.79267454,  0.45407335],\n",
       "       [ 0.89215047, -0.4607342 ],\n",
       "       [ 0.68923791, -0.86100768],\n",
       "       [ 0.67211516,  1.01025908],\n",
       "       [-1.03602501,  1.28966211],\n",
       "       [-1.16396177, -1.11494609],\n",
       "       [-0.78867697, -0.96297813],\n",
       "       [-0.83299872, -0.86290236],\n",
       "       [-1.43077486, -1.10252522],\n",
       "       [-0.68880931,  0.4235397 ],\n",
       "       [ 1.08825945,  1.08672305],\n",
       "       [ 1.05012256,  0.96969537],\n",
       "       [ 0.84380245,  0.66665709],\n",
       "       [-0.86261045,  1.28529815],\n",
       "       [-0.62526666,  0.91775287],\n",
       "       [ 0.50492303, -0.48910994],\n",
       "       [-0.93993545,  1.23421493],\n",
       "       [ 1.23066997, -1.32107606],\n",
       "       [-0.62177499, -0.67674054],\n",
       "       [ 1.12538778,  1.57023046],\n",
       "       [-1.26189525, -0.98725403],\n",
       "       [-1.40072653, -0.6048692 ],\n",
       "       [ 0.8472521 ,  0.84677104],\n",
       "       [-0.76659512,  0.98413322],\n",
       "       [ 1.39358255,  0.89885128],\n",
       "       [ 1.05813478, -1.72886701],\n",
       "       [ 0.70820653, -1.58103837],\n",
       "       [ 1.02160337, -0.38812841],\n",
       "       [-0.94605561, -0.67205637],\n",
       "       [ 1.22143531,  0.91090256],\n",
       "       [-0.92462197,  0.8246898 ],\n",
       "       [-1.23611964, -1.25826368],\n",
       "       [-1.14807803,  1.23742023],\n",
       "       [-0.64578336, -0.85013997],\n",
       "       [ 0.5624813 ,  0.88518161],\n",
       "       [ 1.52071773, -0.98096163],\n",
       "       [ 1.07306648,  1.10982001],\n",
       "       [ 0.86347615,  0.96265497],\n",
       "       [-0.82869675,  0.87698127],\n",
       "       [ 0.77965488,  1.07603089],\n",
       "       [-1.1983807 ,  0.89141679],\n",
       "       [-0.66636314,  0.61320702],\n",
       "       [ 0.88833407, -0.95289594],\n",
       "       [-0.74850041,  1.43088832],\n",
       "       [ 0.8990298 , -1.22755374],\n",
       "       [ 0.76989694, -0.82102396],\n",
       "       [ 1.14099061,  1.3802478 ],\n",
       "       [-0.88668791, -1.13882189],\n",
       "       [ 0.59443927,  0.6366374 ],\n",
       "       [-1.3295933 , -1.3316506 ],\n",
       "       [-0.21453286,  1.3590029 ],\n",
       "       [-1.04919006, -0.47370353],\n",
       "       [-1.23236515, -1.27559005],\n",
       "       [ 1.52892631, -1.33181947],\n",
       "       [-1.1805469 ,  1.07089355],\n",
       "       [-1.26048735, -0.89239673],\n",
       "       [ 1.58206136, -0.72482594],\n",
       "       [ 0.79974305,  1.1274764 ],\n",
       "       [ 1.25298035, -1.0836163 ],\n",
       "       [ 0.96132744,  0.98959504],\n",
       "       [-0.67427292,  1.05586289],\n",
       "       [-1.398488  ,  0.69135508],\n",
       "       [ 1.23900852,  0.73885874],\n",
       "       [-1.09077811, -1.32316248],\n",
       "       [ 0.91802944, -1.02370847],\n",
       "       [-0.91833373, -1.2751192 ],\n",
       "       [-0.97336688, -1.21239787],\n",
       "       [ 1.08363864, -0.94589447],\n",
       "       [ 1.04539183, -1.39273995],\n",
       "       [-0.91983531,  0.99008763]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dental-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 2, 2, 0, 0, 3, 1, 0, 3, 2, 2, 0, 3, 1, 1, 1, 1, 3, 0, 0,\n",
       "       0, 3, 3, 2, 3, 2, 1, 0, 1, 1, 0, 3, 0, 2, 2, 2, 1, 0, 3, 1, 3, 1,\n",
       "       0, 2, 0, 0, 3, 0, 3, 3, 2, 3, 2, 2, 0, 1, 0, 1, 3, 1, 1, 2, 3, 1,\n",
       "       2, 0, 2, 0, 3, 3, 0, 1, 2, 1, 1, 2, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 만든 레이블 4개짜리는 두개로 만들어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mental-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨(정답)을 달아주는것. \n",
    "\n",
    "def label_map(y_,from_,to_):\n",
    "    y = np.copy(y_)\n",
    "    for f in from_:\n",
    "        y[y_==f] = to_\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "parallel-terror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "virgin-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,1 은 0으로 라벨링, 2,3은 1으로 라벨링\n",
    "y_train = label_map(y_train, [0,1], 0)\n",
    "y_train = label_map(y_train, [2,3], 1)\n",
    "y_test = label_map(y_test, [0,1], 0)\n",
    "y_test = label_map(y_test, [2,3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "distributed-angel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "facial-cycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def vis_data(x,y = None, c = 'r'):\n",
    "    if y is None:\n",
    "        y = [None] * len(x)\n",
    "    for x_, y_ in zip(x,y):\n",
    "        if y_ is None:\n",
    "            plt.plot(x_[0], x_[1], '*',markerfacecolor='none', markeredgecolor=c)\n",
    "        else:\n",
    "            plt.plot(x_[0], x_[1], c+'o' if y_ == 0 else c+'+')\n",
    "\n",
    "plt.figure()\n",
    "vis_data(x_train, y_train, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "light-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이 형식으로 된 데이터를 파이토치 텐서 데이터로 변환\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "divine-underwear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8266,  0.7294],\n",
       "        [ 1.2467, -1.1364],\n",
       "        [-1.1635, -0.9240],\n",
       "        [ 1.5884, -1.1244],\n",
       "        [ 0.9567, -0.9242],\n",
       "        [ 0.7411,  1.0389],\n",
       "        [ 1.6495,  0.7672],\n",
       "        [-0.9231,  1.3186],\n",
       "        [-0.8616, -1.3324],\n",
       "        [ 0.9407,  1.2683],\n",
       "        [-0.7927,  0.4541],\n",
       "        [ 0.8922, -0.4607],\n",
       "        [ 0.6892, -0.8610],\n",
       "        [ 0.6721,  1.0103],\n",
       "        [-1.0360,  1.2897],\n",
       "        [-1.1640, -1.1149],\n",
       "        [-0.7887, -0.9630],\n",
       "        [-0.8330, -0.8629],\n",
       "        [-1.4308, -1.1025],\n",
       "        [-0.6888,  0.4235],\n",
       "        [ 1.0883,  1.0867],\n",
       "        [ 1.0501,  0.9697],\n",
       "        [ 0.8438,  0.6667],\n",
       "        [-0.8626,  1.2853],\n",
       "        [-0.6253,  0.9178],\n",
       "        [ 0.5049, -0.4891],\n",
       "        [-0.9399,  1.2342],\n",
       "        [ 1.2307, -1.3211],\n",
       "        [-0.6218, -0.6767],\n",
       "        [ 1.1254,  1.5702],\n",
       "        [-1.2619, -0.9873],\n",
       "        [-1.4007, -0.6049],\n",
       "        [ 0.8473,  0.8468],\n",
       "        [-0.7666,  0.9841],\n",
       "        [ 1.3936,  0.8989],\n",
       "        [ 1.0581, -1.7289],\n",
       "        [ 0.7082, -1.5810],\n",
       "        [ 1.0216, -0.3881],\n",
       "        [-0.9461, -0.6721],\n",
       "        [ 1.2214,  0.9109],\n",
       "        [-0.9246,  0.8247],\n",
       "        [-1.2361, -1.2583],\n",
       "        [-1.1481,  1.2374],\n",
       "        [-0.6458, -0.8501],\n",
       "        [ 0.5625,  0.8852],\n",
       "        [ 1.5207, -0.9810],\n",
       "        [ 1.0731,  1.1098],\n",
       "        [ 0.8635,  0.9627],\n",
       "        [-0.8287,  0.8770],\n",
       "        [ 0.7797,  1.0760],\n",
       "        [-1.1984,  0.8914],\n",
       "        [-0.6664,  0.6132],\n",
       "        [ 0.8883, -0.9529],\n",
       "        [-0.7485,  1.4309],\n",
       "        [ 0.8990, -1.2276],\n",
       "        [ 0.7699, -0.8210],\n",
       "        [ 1.1410,  1.3802],\n",
       "        [-0.8867, -1.1388],\n",
       "        [ 0.5944,  0.6366],\n",
       "        [-1.3296, -1.3317],\n",
       "        [-0.2145,  1.3590],\n",
       "        [-1.0492, -0.4737],\n",
       "        [-1.2324, -1.2756],\n",
       "        [ 1.5289, -1.3318],\n",
       "        [-1.1805,  1.0709],\n",
       "        [-1.2605, -0.8924],\n",
       "        [ 1.5821, -0.7248],\n",
       "        [ 0.7997,  1.1275],\n",
       "        [ 1.2530, -1.0836],\n",
       "        [ 0.9613,  0.9896],\n",
       "        [-0.6743,  1.0559],\n",
       "        [-1.3985,  0.6914],\n",
       "        [ 1.2390,  0.7389],\n",
       "        [-1.0908, -1.3232],\n",
       "        [ 0.9180, -1.0237],\n",
       "        [-0.9183, -1.2751],\n",
       "        [-0.9734, -1.2124],\n",
       "        [ 1.0836, -0.9459],\n",
       "        [ 1.0454, -1.3927],\n",
       "        [-0.9198,  0.9901]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "worse-marijuana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-details",
   "metadata": {},
   "source": [
    "#### 신경망 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "affiliated-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        # input size와 hidden size를 받아 구현하겠다 표현\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size= hidden_size\n",
    "        \n",
    "        # 신경망을 통해 계산되는 과정(연산) 정의\n",
    "        self.linear_1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear_2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    # 차례로 실행하기 위해 forward()이용\n",
    "    def forward(self, input_tensor):\n",
    "        linear_1 = self.linear_1(input_tensor)\n",
    "        relu=self.relu(linear_1)\n",
    "        linear_2 = self.linear_2(relu)\n",
    "        out=self.sigmoid(linear_2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "stunning-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(2, 5)\n",
    "learning_rate = 0.03\n",
    "criterion = torch.nn.BCELoss()\n",
    "epochs = 20000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "nominated-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training, test loss is 0.7599660158157349\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss_before = criterion(model(x_test).squeeze(), y_test)\n",
    "print('Before Training, test loss is {}'.format(test_loss_before.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aware-toner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 0 is 0.7566346526145935\n",
      "Train loss at 100 is 0.22099480032920837\n",
      "Train loss at 200 is 0.11250142008066177\n",
      "Train loss at 300 is 0.06797092407941818\n",
      "Train loss at 400 is 0.04569350928068161\n",
      "Train loss at 500 is 0.032953985035419464\n",
      "Train loss at 600 is 0.02483646385371685\n",
      "Train loss at 700 is 0.01947719417512417\n",
      "Train loss at 800 is 0.015756256878376007\n",
      "Train loss at 900 is 0.01303078979253769\n",
      "Train loss at 1000 is 0.010914446786046028\n",
      "Train loss at 1100 is 0.009302929043769836\n",
      "Train loss at 1200 is 0.008009563200175762\n",
      "Train loss at 1300 is 0.006961470004171133\n",
      "Train loss at 1400 is 0.006108137778937817\n",
      "Train loss at 1500 is 0.005421134177595377\n",
      "Train loss at 1600 is 0.004848630167543888\n",
      "Train loss at 1700 is 0.004370605573058128\n",
      "Train loss at 1800 is 0.003971806727349758\n",
      "Train loss at 1900 is 0.003634932916611433\n",
      "Train loss at 2000 is 0.003346924902871251\n",
      "Train loss at 2100 is 0.0030981171876192093\n",
      "Train loss at 2200 is 0.0028812133241444826\n",
      "Train loss at 2300 is 0.002690019551664591\n",
      "Train loss at 2400 is 0.0025208282750099897\n",
      "Train loss at 2500 is 0.0023701777681708336\n",
      "Train loss at 2600 is 0.0022352507803589106\n",
      "Train loss at 2700 is 0.002113731810823083\n",
      "Train loss at 2800 is 0.002003813860937953\n",
      "Train loss at 2900 is 0.0019039526814594865\n",
      "Train loss at 3000 is 0.0018128659576177597\n",
      "Train loss at 3100 is 0.0017294876743108034\n",
      "Train loss at 3200 is 0.0016529023414477706\n",
      "Train loss at 3300 is 0.0015823270659893751\n",
      "Train loss at 3400 is 0.0015171112027019262\n",
      "Train loss at 3500 is 0.0014566805912181735\n",
      "Train loss at 3600 is 0.001400538021698594\n",
      "Train loss at 3700 is 0.0013482494978234172\n",
      "Train loss at 3800 is 0.0012994497083127499\n",
      "Train loss at 3900 is 0.001253806403838098\n",
      "Train loss at 4000 is 0.0012110404204577208\n",
      "Train loss at 4100 is 0.0011708838865160942\n",
      "Train loss at 4200 is 0.0011331227142363787\n",
      "Train loss at 4300 is 0.0010975534096360207\n",
      "Train loss at 4400 is 0.0010639940155670047\n",
      "Train loss at 4500 is 0.0010322778252884746\n",
      "Train loss at 4600 is 0.0010022750357165933\n",
      "Train loss at 4700 is 0.0009738385560922325\n",
      "Train loss at 4800 is 0.0009468600037507713\n",
      "Train loss at 4900 is 0.0009212386794388294\n",
      "Train loss at 5000 is 0.0008968672482296824\n",
      "Train loss at 5100 is 0.0008736656163819134\n",
      "Train loss at 5200 is 0.0008515532244928181\n",
      "Train loss at 5300 is 0.0008304522489197552\n",
      "Train loss at 5400 is 0.0008103059371933341\n",
      "Train loss at 5500 is 0.0007910375716164708\n",
      "Train loss at 5600 is 0.0007726028561592102\n",
      "Train loss at 5700 is 0.0007549480069428682\n",
      "Train loss at 5800 is 0.0007380297756753862\n",
      "Train loss at 5900 is 0.0007218019454739988\n",
      "Train loss at 6000 is 0.0007062217919155955\n",
      "Train loss at 6100 is 0.0006912557873874903\n",
      "Train loss at 6200 is 0.0006768614402972162\n",
      "Train loss at 6300 is 0.0006630191346630454\n",
      "Train loss at 6400 is 0.0006496940040960908\n",
      "Train loss at 6500 is 0.0006368561880663037\n",
      "Train loss at 6600 is 0.0006244808319024742\n",
      "Train loss at 6700 is 0.0006125457002781332\n",
      "Train loss at 6800 is 0.000601028383243829\n",
      "Train loss at 6900 is 0.0005899094976484776\n",
      "Train loss at 7000 is 0.0005791635485365987\n",
      "Train loss at 7100 is 0.0005687749362550676\n",
      "Train loss at 7200 is 0.000558731029741466\n",
      "Train loss at 7300 is 0.0005490145413205028\n",
      "Train loss at 7400 is 0.000539602420758456\n",
      "Train loss at 7500 is 0.0005304849473759532\n",
      "Train loss at 7600 is 0.0005216525169089437\n",
      "Train loss at 7700 is 0.0005130922072567046\n",
      "Train loss at 7800 is 0.0005047833546996117\n",
      "Train loss at 7900 is 0.0004967228742316365\n",
      "Train loss at 8000 is 0.0004889084375463426\n",
      "Train loss at 8100 is 0.00048131035873666406\n",
      "Train loss at 8200 is 0.00047392636770382524\n",
      "Train loss at 8300 is 0.0004667556786444038\n",
      "Train loss at 8400 is 0.0004597796068992466\n",
      "Train loss at 8500 is 0.0004529960569925606\n",
      "Train loss at 8600 is 0.00044639880070462823\n",
      "Train loss at 8700 is 0.00043997238390147686\n",
      "Train loss at 8800 is 0.0004337205027695745\n",
      "Train loss at 8900 is 0.0004276334657333791\n",
      "Train loss at 9000 is 0.0004217022506054491\n",
      "Train loss at 9100 is 0.00041591719491407275\n",
      "Train loss at 9200 is 0.0004102887469343841\n",
      "Train loss at 9300 is 0.0004047923139296472\n",
      "Train loss at 9400 is 0.0003994368016719818\n",
      "Train loss at 9500 is 0.00039420503890141845\n",
      "Train loss at 9600 is 0.00038911279989406466\n",
      "Train loss at 9700 is 0.0003841293801087886\n",
      "Train loss at 9800 is 0.0003792675561271608\n",
      "Train loss at 9900 is 0.00037452197284437716\n",
      "Train loss at 10000 is 0.00036987787461839616\n",
      "Train loss at 10100 is 0.00036534335231408477\n",
      "Train loss at 10200 is 0.00036091325455345213\n",
      "Train loss at 10300 is 0.00035657556145451963\n",
      "Train loss at 10400 is 0.00035234529059380293\n",
      "Train loss at 10500 is 0.0003482014872133732\n",
      "Train loss at 10600 is 0.0003441464505158365\n",
      "Train loss at 10700 is 0.00034018378937616944\n",
      "Train loss at 10800 is 0.00033629947574809194\n",
      "Train loss at 10900 is 0.0003324993886053562\n",
      "Train loss at 11000 is 0.0003287753788754344\n",
      "Train loss at 11100 is 0.0003251363814342767\n",
      "Train loss at 11200 is 0.0003215674078091979\n",
      "Train loss at 11300 is 0.0003180685453116894\n",
      "Train loss at 11400 is 0.0003146420349366963\n",
      "Train loss at 11500 is 0.00031128633418120444\n",
      "Train loss at 11600 is 0.00030799474916420877\n",
      "Train loss at 11700 is 0.0003047612844966352\n",
      "Train loss at 11800 is 0.0003015986585523933\n",
      "Train loss at 11900 is 0.0002984986931551248\n",
      "Train loss at 12000 is 0.00029545387951657176\n",
      "Train loss at 12100 is 0.00029246724443510175\n",
      "Train loss at 12200 is 0.0002895342477131635\n",
      "Train loss at 12300 is 0.0002866601280402392\n",
      "Train loss at 12400 is 0.00028384049073792994\n",
      "Train loss at 12500 is 0.000281064014416188\n",
      "Train loss at 12600 is 0.0002783428062684834\n",
      "Train loss at 12700 is 0.0002756714529823512\n",
      "Train loss at 12800 is 0.0002730426494963467\n",
      "Train loss at 12900 is 0.00027046751347370446\n",
      "Train loss at 13000 is 0.000267931871348992\n",
      "Train loss at 13100 is 0.00026544020511209965\n",
      "Train loss at 13200 is 0.00026299775345250964\n",
      "Train loss at 13300 is 0.0002605918562039733\n",
      "Train loss at 13400 is 0.00025822545285336673\n",
      "Train loss at 13500 is 0.00025589781580492854\n",
      "Train loss at 13600 is 0.0002536186366342008\n",
      "Train loss at 13700 is 0.00025137298507615924\n",
      "Train loss at 13800 is 0.0002491601335350424\n",
      "Train loss at 13900 is 0.0002469868049956858\n",
      "Train loss at 14000 is 0.00024484554887749255\n",
      "Train loss at 14100 is 0.0002427348226774484\n",
      "Train loss at 14200 is 0.000240668814512901\n",
      "Train loss at 14300 is 0.00023862968373578042\n",
      "Train loss at 14400 is 0.000236622552620247\n",
      "Train loss at 14500 is 0.00023464453988708556\n",
      "Train loss at 14600 is 0.00023270452220458537\n",
      "Train loss at 14700 is 0.000230789853958413\n",
      "Train loss at 14800 is 0.00022890503169037402\n",
      "Train loss at 14900 is 0.00022704550065100193\n",
      "Train loss at 15000 is 0.00022521731443703175\n",
      "Train loss at 15100 is 0.00022342114243656397\n",
      "Train loss at 15200 is 0.000221642927499488\n",
      "Train loss at 15300 is 0.0002198937290813774\n",
      "Train loss at 15400 is 0.00021817139349877834\n",
      "Train loss at 15500 is 0.00021647437824867666\n",
      "Train loss at 15600 is 0.00021480419673025608\n",
      "Train loss at 15700 is 0.0002131519140675664\n",
      "Train loss at 15800 is 0.00021152719273231924\n",
      "Train loss at 15900 is 0.00020992712234146893\n",
      "Train loss at 16000 is 0.00020834864699281752\n",
      "Train loss at 16100 is 0.00020678728469647467\n",
      "Train loss at 16200 is 0.00020525426953099668\n",
      "Train loss at 16300 is 0.00020373689767438918\n",
      "Train loss at 16400 is 0.00020224114996381104\n",
      "Train loss at 16500 is 0.00020076702639926225\n",
      "Train loss at 16600 is 0.0001993189798668027\n",
      "Train loss at 16700 is 0.00019788285135291517\n",
      "Train loss at 16800 is 0.0001964698312804103\n",
      "Train loss at 16900 is 0.00019507025717757642\n",
      "Train loss at 17000 is 0.00019369598885532469\n",
      "Train loss at 17100 is 0.00019233665079809725\n",
      "Train loss at 17200 is 0.0001909974089358002\n",
      "Train loss at 17300 is 0.00018966940115205944\n",
      "Train loss at 17400 is 0.0001883644436020404\n",
      "Train loss at 17500 is 0.00018707741401158273\n",
      "Train loss at 17600 is 0.00018579556490294635\n",
      "Train loss at 17700 is 0.00018454506061971188\n",
      "Train loss at 17800 is 0.00018329899467062205\n",
      "Train loss at 17900 is 0.00018207829270977527\n",
      "Train loss at 18000 is 0.00018086428462993354\n",
      "Train loss at 18100 is 0.00017966743325814605\n",
      "Train loss at 18200 is 0.00017848325660452247\n",
      "Train loss at 18300 is 0.00017732066044118255\n",
      "Train loss at 18400 is 0.0001761677849572152\n",
      "Train loss at 18500 is 0.00017502609989605844\n",
      "Train loss at 18600 is 0.00017390451102983207\n",
      "Train loss at 18700 is 0.00017279561143368483\n",
      "Train loss at 18800 is 0.00017169563216157258\n",
      "Train loss at 18900 is 0.00017060982645489275\n",
      "Train loss at 19000 is 0.00016953669546637684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 19100 is 0.00016848069208208472\n",
      "Train loss at 19200 is 0.00016743291052989662\n",
      "Train loss at 19300 is 0.00016639554814901203\n",
      "Train loss at 19400 is 0.00016537235933355987\n",
      "Train loss at 19500 is 0.00016435884754173458\n",
      "Train loss at 19600 is 0.00016335352847818285\n",
      "Train loss at 19700 is 0.00016237057570833713\n",
      "Train loss at 19800 is 0.0001613913627807051\n",
      "Train loss at 19900 is 0.00016042629431467503\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_output = model(x_train)\n",
    "    train_loss = criterion(train_output.squeeze(), y_train)\n",
    "    if epoch % 100 == 0:\n",
    "        print('Train loss at {} is {}'.format(epoch, train_loss.item()))\n",
    "    train_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "southwest-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Training, test loss is 0.00016796424461062998\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = criterion(torch.squeeze(model(x_test)), y_test)\n",
    "print('After Training, test loss is {}'.format(test_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "arranged-dairy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict format of the model: OrderedDict([('linear_1.weight', tensor([[ 0.0133,  0.5506],\n",
      "        [ 0.7989, -0.0728],\n",
      "        [-0.9335, -0.6839],\n",
      "        [-1.1917,  0.2595],\n",
      "        [ 0.0134,  1.0636]])), ('linear_1.bias', tensor([-6.5182e-01,  2.0072e+00,  9.6529e-04,  8.6490e-01,  4.4117e-01])), ('linear_2.weight', tensor([[ 0.2960, -2.0151, -0.9477, -1.1844, -0.9844]])), ('linear_2.bias', tensor([-2.3916]))])\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './model.pt')\n",
    "print('state_dict format of the model: {}'.format(model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "excess-mouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 [-1, 1]이 레이블 1을 가질 확률은 0.00010849133104784414\n"
     ]
    }
   ],
   "source": [
    "new_model = NeuralNet(2, 5)\n",
    "new_model.load_state_dict(torch.load('./model.pt'))\n",
    "new_model.eval()\n",
    "print('벡터 [-1, 1]이 레이블 1을 가질 확률은 {}'.format(new_model(torch.FloatTensor([-1,1])).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-remains",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
